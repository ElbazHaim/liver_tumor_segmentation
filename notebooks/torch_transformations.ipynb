{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "unprocessed_data_root_dir = Path(\"/home/haim/code/tumors/data\")\n",
    "processed_data_root_dir = Path(\"/home/haim/code/tumors/data/processed\")\n",
    "processed_images_dir: Path = processed_data_root_dir / \"volumes\"\n",
    "processed_segments_dir: Path = processed_data_root_dir / \"segmentations\"\n",
    "\n",
    "df = pd.read_csv(\"/home/haim/code/tumors/liver_tumors/image_and_segment_paths.csv\")\n",
    "vol = 0\n",
    "idx = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = nib.load(unprocessed_data_root_dir / df[\"image_path\"][vol])\n",
    "img_data = img.get_fdata()\n",
    "print(img_data.shape)\n",
    "plt.imshow(img_data[:, :, idx].T, cmap=\"bone\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/haim/code/tumors/liver_tumors/src/liver_tk/transforms/transforms.py\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "\n",
    "class PadOrTrim:\n",
    "    \"\"\"\n",
    "    A torch transform that Adjusts the depth of the input image and segment to match the target\n",
    "    depth by either padding or trimming.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input image tensor.\n",
    "        segment (torch.Tensor): The input segmentation tensor.\n",
    "        target_depth (int): The target depth to which the image and segment should be adjusted.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The adjusted image and segmentation tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, target_depth: int, sparse_result: bool = False, depth_dim: int = 2, pad: float = 0):\n",
    "        self.target_depth = target_depth\n",
    "        self.depth_dim = depth_dim\n",
    "        self.pad = pad\n",
    "        self.sparse_result = sparse_result\n",
    "\n",
    "    def __call__(self, sample: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        current_depth = sample.shape[self.depth_dim]\n",
    "\n",
    "        if current_depth > self.target_depth:\n",
    "            sample = sample[:, :, :self.target_depth]\n",
    "        elif current_depth < self.target_depth:\n",
    "            pad_after = self.target_depth - current_depth\n",
    "            sample = torch.nn.functional.pad(sample, (self.pad, pad_after))\n",
    "\n",
    "        return sample\n",
    "\n",
    "class WindowImage:\n",
    "    \"\"\"\n",
    "    A torch transform tha applies Hounsfiled windowing to a CT image for better visualization.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input CT image as a PyTorch tensor.\n",
    "        window_level (float): The center of the windowing range.\n",
    "        window_width (float): The width of the windowing range.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The windowed image normalized to the range [0, 255] as a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, window_level: float, window_width: float):\n",
    "        self.window_level = window_level\n",
    "        self.window_width = window_width\n",
    "\n",
    "    def __call__(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        min_intensity = self.window_level - (self.window_width / 2)\n",
    "        max_intensity = self.window_level + (self.window_width / 2)\n",
    "        \n",
    "        windowed_image = torch.clamp(image, min_intensity, max_intensity)\n",
    "        windowed_image = ((windowed_image - min_intensity) / (max_intensity - min_intensity) * 255).to(torch.uint8)\n",
    "        \n",
    "        return windowed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from liver_tk.transforms import PadOrTrim, WindowImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_level: int = 30\n",
    "window_width: int = 150\n",
    "target_depth: int = 851\n",
    "image_transforms = transforms.Compose([\n",
    "        WindowImage(window_level=window_level, window_width=window_width),\n",
    "        PadOrTrim(target_depth=target_depth),\n",
    "        # transforms.Lambda(lambda x: x.to_sparse(2)),\n",
    "    ])\n",
    "mask_transforms = transforms.Compose([PadOrTrim(target_depth=target_depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/haim/code/tumors/liver_tumors/src/liver_tk/datamodule/segmentation_dataset.py\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, data_root_path: Path, data_frame: pd.DataFrame, image_transform: Optional[torch.nn.Module] = None, mask_transform: Optional[torch.nn.Module] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_root_path (Path): Root directory path for the dataset.\n",
    "            data_frame (pd.DataFrame): DataFrame with volume, image paths, and segmentation paths.\n",
    "            image_transform (Optional[torch.nn.Module], optional): Optional transform to be applied on an image.\n",
    "            mask_transform (Optional[torch.nn.Module], optional): Optional transform to be applied on a mask.\n",
    "        \"\"\"\n",
    "        self.data_root_path = data_root_path\n",
    "        self.data_frame = data_frame\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.data_root_path / self.data_frame.iloc[idx, 1]\n",
    "        segment_path = self.data_root_path / self.data_frame.iloc[idx, 2]\n",
    "\n",
    "        img = nib.load(img_path).get_fdata()\n",
    "        segment = nib.load(segment_path).get_fdata()\n",
    "\n",
    "        # Convert to torch tensor\n",
    "        img = torch.tensor(img, dtype=torch.float16)\n",
    "        segment = torch.tensor(segment, dtype=torch.uint8)\n",
    "\n",
    "        if self.image_transform:\n",
    "            img = self.image_transform(img)\n",
    "\n",
    "        if self.mask_transform:\n",
    "            segment = self.mask_transform(segment)\n",
    "\n",
    "        return img, segment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "data_frame = pd.read_csv(\"/home/haim/code/tumors/liver_tumors/image_and_segment_paths.csv\")  # Load your DataFrame\n",
    "\n",
    "dataset = SegmentationDataset(\n",
    "    data_root_path=Path(\"/home/haim/code/tumors/data\"),\n",
    "    data_frame=data_frame,\n",
    "    image_transform=image_transforms,\n",
    "    mask_transform=mask_transforms,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X.numpy()[0, :, :, 85].T, cmap=\"bone\")\n",
    "plt.imshow(y.numpy()[0, :, :, 85].T, cmap=\"viridis\", alpha=0.3)\n",
    "plt.axis('off')\n",
    "plt.title(\"Segmented Liver\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.numpy()[0, :, :, 85].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/haim/code/tumors/liver_tumors/src/liver_tk/datamodule/segmentation_datamodule.py\n",
    "import lightning as L\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class SegmentationDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_root_path: str, csv_file_path: str, batch_size: int, num_workers: int = 4, window_level: int = 30, window_width: int = 150, target_depth: int = 851):\n",
    "        super().__init__()\n",
    "        self.data_root_path = Path(data_root_path)\n",
    "        self.csv_file_path = Path(csv_file_path)\n",
    "        self.batch_size: int = batch_size\n",
    "        self.num_workers: int = num_workers\n",
    "\n",
    "        self.df_train: pd.DataFrame = None\n",
    "        self.df_val: pd.DataFrame = None\n",
    "        self.df_test: pd.DataFrame = None\n",
    "\n",
    "        self.image_transform = transforms.Compose([\n",
    "            WindowImage(window_level=window_level, window_width=window_width),\n",
    "            PadOrTrim(target_depth=target_depth),\n",
    "        ])\n",
    "\n",
    "        self.mask_transform = transforms.Compose([PadOrTrim(target_depth=target_depth)])\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        train_size: int = 0.7\n",
    "        val_size: int = 0.15\n",
    "        test_size: int = 0.15\n",
    "        df = pd.read_csv(self.csv_file)\n",
    "        self.df_train, df_temp = train_test_split(df, test_size=(1.0 - train_size), random_state=42)\n",
    "        test_frac = test_size / (val_size + test_size)\n",
    "        self.df_val, self.df_test = train_test_split(df_temp, test_size=test_frac, random_state=42)\n",
    "\n",
    "\n",
    "        self.train_set = SegmentationDataset(\n",
    "            data_root_path=self.data_root_path,\n",
    "            data_frame=self.df_train,\n",
    "            image_transform=image_transforms,\n",
    "            mask_transform=mask_transforms,\n",
    "        )\n",
    "\n",
    "        self.val_set = SegmentationDataset(\n",
    "            data_root_path=self.data_root_path,\n",
    "            data_frame=self.df_val,\n",
    "            image_transform=image_transforms,\n",
    "            mask_transform=mask_transforms,\n",
    "        )\n",
    "\n",
    "        self.test_set = SegmentationDataset(\n",
    "            data_root_path=self.data_root_path,\n",
    "            data_frame=self.df_test,\n",
    "            image_transform=image_transforms,\n",
    "            mask_transform=mask_transforms,\n",
    "        )\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_size = 0.7\n",
    "# val_size = 0.15\n",
    "# test_size = 0.15\n",
    "# df = pd.read_csv(\"/home/haim/code/tumors/liver_tumors/image_and_segment_paths.csv\")\n",
    "# df_train, df_temp = train_test_split(df, test_size=(1.0 - train_size), random_state=42)\n",
    "# val_frac = val_size / (val_size + test_size)\n",
    "# test_frac = test_size / (val_size + test_size)\n",
    "# df_val, df_test = train_test_split(df_temp, test_size=test_frac, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liver_tk.datamodule.segmentation_datamodule import SegmentationDataModule\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = SegmentationDataModule(\n",
    "        batch_size=1,\n",
    "        data_root_path=\"/home/haim/code/tumors/data\",\n",
    "        csv_file_path=\"/home/haim/code/tumors/liver_tumors/image_and_segment_paths.csv\",\n",
    "        num_workers=1,\n",
    ")\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = datamodule.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(\"cuda\")\n",
    "y = y.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "ic(X.shape)\n",
    "ic(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from liver_tk.nets.unet import SegmentationModel\n",
    "from liver_tk.datamodule.segmentation_datamodule import SegmentationDataModule\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "model = SegmentationModel().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, D, H, W\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N, Cin, H, D\n",
    "X.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512, 852])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
