{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"/home/haim/code/tumors/data/\")\n",
    "filepath = Path(\"/home/haim/code/tumors/data/volumes/volume-0.nii\")\n",
    "\n",
    "img = nib.load(filepath)\n",
    "img_data = img.get_fdata()\n",
    "print(img_data.shape)\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 73\n",
    "# plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img.to(\"cpu\")[:, :, idx].T, cmap=\"gray\")\n",
    "# plt.imshow(mask[:, :, idx].T, cmap=\"viridis\", alpha=0.3)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 834\n",
    "# plt.figure(figsize=(10, 10))\n",
    "plt.imshow(img2.to(\"cpu\")[:, :, idx].T, cmap=\"gray\")\n",
    "# plt.imshow(mask[:, :, idx].T, cmap=\"viridis\", alpha=0.3)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "img = torch.tensor(img_data, dtype=torch.float, device=\"cuda\")\n",
    "\n",
    "def resize_3d(image, target_depth):\n",
    "    height, width, depth = image.shape\n",
    "    image = image.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n",
    "    resized_image = F.interpolate(image, size=(height, width, target_depth), mode='trilinear', align_corners=False)\n",
    "    return resized_image.squeeze(0).squeeze(0)\n",
    "\n",
    "\n",
    "img2 = resize_3d(img, target_depth=864)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet\n",
    "\n",
    "net = UNet(\n",
    "    spatial_dims=3,       \n",
    "    in_channels=1,        \n",
    "    out_channels=1,       \n",
    "    channels=(16, 32, 64),\n",
    "    strides=(2, 2),       \n",
    "    kernel_size=3,        \n",
    "    up_kernel_size=3,     \n",
    "    dropout=0.1,          \n",
    "\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(net) / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2.to(\"cuda\").unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.randn(1, 1, 512, 512, 864).to(\"cuda\")  # Adjust input size if needed\n",
    "    output_tensor = net(input_tensor)\n",
    "print(\"Output tensor shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = net(img2.to(\"cuda\").unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import mlflow.pytorch\n",
    "from monai.networks.nets import UNet\n",
    "from torchmetrics import MetricCollection, Accuracy\n",
    "from torchmetrics.detection.iou import IntersectionOverUnion\n",
    "# from mlflow import log_metric\n",
    "\n",
    "\n",
    "class SegmentationModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        out_channels=1,\n",
    "        channels=(16, 32, 64, 128),\n",
    "        strides=(2, 2, 2),\n",
    "        lr=1e-3,\n",
    "    ):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            channels=(16, 32, 64),\n",
    "            strides=(2, 2),\n",
    "            kernel_size=3,\n",
    "            up_kernel_size=3,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        metrics = MetricCollection(\n",
    "            {\"IoU\": IntersectionOverUnion(num_classes=2), \"Accuracy\": Accuracy()}\n",
    "        )\n",
    "        self.train_metrics = metrics.clone(prefix=\"train_\")\n",
    "        self.val_metrics = metrics.clone(prefix=\"val_\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.train_metrics(preds, y)\n",
    "        self.log_dict(self.train_metrics, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        preds = torch.argmax(y_hat, dim=1)\n",
    "        self.val_metrics(preds, y)\n",
    "        self.log_dict(self.val_metrics, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "    # def on_train_epoch_end(self):\n",
    "    #     metrics = self.train_metrics.compute()\n",
    "    #     for k, v in metrics.items():\n",
    "    #         log_metric(k, v)\n",
    "    #     self.train_metrics.reset()\n",
    "\n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     metrics = self.val_metrics.compute()\n",
    "    #     for k, v in metrics.items():\n",
    "    #         log_metric(k, v)\n",
    "    #     self.val_metrics.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
